{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaOtel26zyNe"
      },
      "source": [
        "# 0.0 Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC-2H0llzyNk"
      },
      "source": [
        "## 1.0 Imports - !pip install <package_name> if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TW2-RkQ7zyNk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/macbookpro/Desktop/UPSaclay Courses/T4/Information retrieval/Assignment/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SjDTn9yMzyNm",
        "outputId": "871f42c5-1de9-4aff-9fcb-651aa933a6a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/macbookpro/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/macbookpro/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')  # For lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0kqKHXMzyNn"
      },
      "source": [
        "## 0.1 Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ds6mtxS2zyNo"
      },
      "outputs": [],
      "source": [
        "def load_json_data(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        contents = json.load(file)\n",
        "    return contents\n",
        "\n",
        "\n",
        "def create_tfidf_matrix(citing_dataset, nonciting_dataset, vectorizer=TfidfVectorizer()):\n",
        "    \"\"\"\n",
        "    Creates TF-IDF matrix for the given citing and non-citing datasets based on the specified text column.\n",
        "\n",
        "    Parameters:\n",
        "    citing_dataset (json)): DataFrame containing citing patents.\n",
        "    nonciting_dataset (json): DataFrame containing non-citing patents.\n",
        "    vectorizer (TfidfVectorizer, optional): TfidfVectorizer object for vectorizing text data.\n",
        "                                             Defaults to TfidfVectorizer().\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing TF-IDF matrices for citing and non-citing patents respectively.\n",
        "           (tfidf_matrix_citing, tfidf_matrix_nonciting)\n",
        "    \"\"\"\n",
        "    all_text = [patent['text'] for patent in citing_dataset + nonciting_dataset]\n",
        "\n",
        "    # Vectorizing descriptions\n",
        "    print(\"Vectorizing descriptions...\")\n",
        "    tfidf_matrix = vectorizer.fit_transform(tqdm(all_text, desc=\"TF-IDF\"))\n",
        "\n",
        "    # Since we're interested in similarities between citing and cited patents,\n",
        "    # we need to split the TF-IDF matrix back into two parts\n",
        "    split_index = len(citing_dataset)\n",
        "    tfidf_matrix_citing = tfidf_matrix[:split_index]\n",
        "    tfidf_matrix_nonciting = tfidf_matrix[split_index:]\n",
        "\n",
        "    # Size of vocabulary\n",
        "    print(\"Size of vocabulary:\", len(vectorizer.vocabulary_))\n",
        "\n",
        "    return tfidf_matrix_citing, tfidf_matrix_nonciting\n",
        "\n",
        "\n",
        "\n",
        "def get_mapping_dict(mapping_df):\n",
        "    \"\"\"\n",
        "    Creates dictionary of citing ids to non-citing id based on given dataframe (which is based on providedjson)\n",
        "\n",
        "    Parameters:\n",
        "    mapping_df (DataFrame): DataFrame containing mapping between citing and cited patents\n",
        "    Returns:\n",
        "    dict: dictionary of unique citing patent ids to list of cited patent ids\n",
        "    \"\"\"\n",
        "    mapping_dict = {}\n",
        "\n",
        "    for _, row in mapping_df.iterrows():\n",
        "        key = row[0]  # Value from column 0\n",
        "        value = row[2]  # Value from column 2\n",
        "        if key in mapping_dict:\n",
        "            mapping_dict[key].append(value)\n",
        "        else:\n",
        "            mapping_dict[key] = [value]\n",
        "\n",
        "    return mapping_dict\n",
        "\n",
        "def create_corpus(corpus, text_type):\n",
        "    \"\"\"\n",
        "    Extracts text data from a corpus based on the specified text type.\n",
        "\n",
        "    Parameters:\n",
        "    corpus (list): List of dictionaries representing patent documents.\n",
        "    text_type (str): Type of text to extract ('title', 'abstract', 'claim1', 'claims', 'description', 'fulltext').\n",
        "\n",
        "    Returns:\n",
        "    list: List of dictionaries with 'id' and 'text' keys representing each document in the corpus.\n",
        "    \"\"\"\n",
        "\n",
        "    app_ids = [doc['Application_Number'] + doc['Application_Category'] for doc in corpus]\n",
        "\n",
        "    cnt = 0 # count the number of documents without text\n",
        "    texts = []  # list of texts\n",
        "    ids_to_remove = []  # list of ids of documents without text, to remove them from the corpus\n",
        "\n",
        "    if text_type == 'title':\n",
        "        for doc in corpus:\n",
        "            try:\n",
        "                texts.append(doc['Content']['title'])\n",
        "            except: # if the document does not have a title\n",
        "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
        "                cnt += 1\n",
        "        print(f\"Number of documents without title: {cnt}\")\n",
        "\n",
        "    elif text_type == 'abstract':\n",
        "        for doc in corpus:\n",
        "            try:\n",
        "                texts.append(doc['Content']['pa01'])\n",
        "            except: # if the document does not have an abstract\n",
        "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
        "                cnt += 1\n",
        "        print(f\"Number of documents without abstract: {cnt}\")\n",
        "\n",
        "    elif text_type == 'claim1':\n",
        "        for doc in corpus:\n",
        "            try:\n",
        "                texts.append(doc['Content']['c-en-0001'])\n",
        "            except: # if the document does not have claim 1\n",
        "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
        "                cnt += 1\n",
        "        print(f\"Number of documents without claim 1: {cnt}\")\n",
        "\n",
        "    elif text_type == 'claims':\n",
        "        # all the values with the key starting with 'c-en-', each element in the final list is a list of claims\n",
        "        for doc in corpus:\n",
        "            doc_claims = []\n",
        "            for key in doc['Content'].keys():\n",
        "                if key.startswith('c-en-'):\n",
        "                    doc_claims.append(doc['Content'][key])\n",
        "            if len(doc_claims) == 0:    # if the document does not have any claims\n",
        "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
        "                cnt += 1\n",
        "            else:\n",
        "                doc_text_string = ' '.join(doc_claims)\n",
        "                texts.append(doc_text_string)\n",
        "        print(f\"Number of documents without claims: {cnt}\")\n",
        "\n",
        "    elif text_type == 'description':\n",
        "        # all the values with the key starting with 'p'\n",
        "        for doc in corpus:\n",
        "            doc_text = []\n",
        "            for key in doc['Content'].keys():\n",
        "                if key.startswith('p'):\n",
        "                    doc_text.append(doc['Content'][key])\n",
        "            if len(doc_text) == 0:  # if the document does not have any description\n",
        "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
        "                cnt += 1\n",
        "            else:\n",
        "                doc_text_string = ' '.join(doc_text)\n",
        "                texts.append(doc_text_string)\n",
        "        print(f\"Number of documents without description: {cnt}\")\n",
        "\n",
        "    elif text_type == 'fulltext':\n",
        "        for doc in corpus:\n",
        "            doc_text = list(doc['Content'].values())\n",
        "            doc_text_string = ' '.join(doc_text)\n",
        "            texts.append(doc_text_string)\n",
        "        if cnt > 0:\n",
        "            print(f\"Number of documents without any text: {cnt}\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid text type\")\n",
        "\n",
        "    if len(ids_to_remove) > 0:\n",
        "        print(f\"Removing {len(ids_to_remove)} documents without required text\")\n",
        "        for id_ in ids_to_remove[::-1]:\n",
        "            idx = app_ids.index(id_)\n",
        "            del app_ids[idx]\n",
        "\n",
        "    # Create a list of dictionaries with app_ids and texts\n",
        "    corpus_data = [{'id': app_id, 'text': text} for app_id, text in zip(app_ids, texts)]\n",
        "\n",
        "    return corpus_data\n",
        "\n",
        "\n",
        "def get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n",
        "    \"\"\"\n",
        "    Get the true and predicted labels for the metrics calculation.\n",
        "\n",
        "    Parameters:\n",
        "    citing_to_cited_dict : dict of str : list of str\n",
        "        Mapping between citing patents and the list of their cited patents\n",
        "    recommendations_dict : dict of str : list of str\n",
        "        Mapping between citing patents and the sorted list of recommended patents\n",
        "\n",
        "    Returns:\n",
        "    list of list\n",
        "        True relevant items for each recommendation list.\n",
        "    list of list\n",
        "        Predicted recommended items for each recommendation list.\n",
        "    int\n",
        "        Number of patents not in the citation mapping\n",
        "    \"\"\"\n",
        "    # Initialize lists to store true labels and predicted labels\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    not_in_citation_mapping = 0\n",
        "\n",
        "    # Iterate over the items in both dictionaries\n",
        "    for citing_id in recommendations_dict.keys():\n",
        "        # Check if the citing_id is present in both dictionaries\n",
        "        if citing_id in citing_to_cited_dict:\n",
        "            # If yes, append the recommended items from both dictionaries to the respective lists\n",
        "            true_labels.append(citing_to_cited_dict[citing_id])\n",
        "            predicted_labels.append(recommendations_dict[citing_id])\n",
        "        else:\n",
        "            not_in_citation_mapping += 1\n",
        "\n",
        "    return true_labels, predicted_labels, not_in_citation_mapping\n",
        "\n",
        "\n",
        "\n",
        "def mean_recall_at_k(true_labels, predicted_labels, k=10):\n",
        "    \"\"\"\n",
        "    Calculate the mean Recall@k for a list of recommendations.\n",
        "\n",
        "    Parameters:\n",
        "    true_labels : list of list\n",
        "        True relevant items for each recommendation list.\n",
        "    predicted_labels : list of list\n",
        "        Predicted recommended items for each recommendation list.\n",
        "    k : int\n",
        "        Number of recommendations to consider.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Mean Recall@k value.\n",
        "    \"\"\"\n",
        "    recalls_at_k = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate Recall@k for each recommendation list\n",
        "        true_set = set(true)\n",
        "        k = min(k, len(pred))\n",
        "        relevant_count = sum(1 for item in pred[:k] if item in true_set)\n",
        "        recalls_at_k.append(relevant_count / len(true_set))\n",
        "\n",
        "    # Calculate the mean Recall@k\n",
        "    mean_recall = sum(recalls_at_k) / len(recalls_at_k)\n",
        "\n",
        "    return mean_recall\n",
        "\n",
        "def mean_inv_ranking(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Calculate the mean of lists of the mean inverse rank of true relevant items\n",
        "    in the lists of sorted recommended items.\n",
        "\n",
        "    Parameters:\n",
        "    true_labels : list of list\n",
        "        True relevant items for each recommendation list.\n",
        "    predicted_labels : list of list\n",
        "        Predicted recommended items for each recommendation list.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Mean of lists of the mean inverse rank of true relevant items.\n",
        "    \"\"\"\n",
        "    mean_ranks = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate the inverse rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        ranks = []\n",
        "        for item in true:\n",
        "            try:\n",
        "                rank = 1 / (pred.index(item) + 1)\n",
        "            except ValueError:\n",
        "                rank = 0  # If item not found, assign 0\n",
        "            ranks.append(rank)\n",
        "\n",
        "        # Calculate the mean inverse rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        mean_rank = sum(ranks) / len(ranks)\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    # Calculate the mean of the mean inverse ranks across all recommendation lists\n",
        "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks)\n",
        "\n",
        "    return mean_of_mean_ranks\n",
        "\n",
        "\n",
        "def mean_ranking(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Calculate the mean of lists of the mean rank of true relevant items\n",
        "    in the lists of sorted recommended items.\n",
        "\n",
        "    Parameters:\n",
        "    true_labels : list of list\n",
        "        True relevant items for each recommendation list.\n",
        "    predicted_labels : list of list\n",
        "        Predicted recommended items for each recommendation list.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Mean of lists of the mean rank of true relevant items.\n",
        "    \"\"\"\n",
        "    mean_ranks = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate the rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        ranks = []\n",
        "        for item in true:\n",
        "            try:\n",
        "                rank = pred.index(item) + 1\n",
        "            except ValueError:\n",
        "                rank = len(pred)  # If item not found, assign the length of the list\n",
        "            ranks.append(rank)\n",
        "\n",
        "        # Calculate the mean rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        mean_rank = sum(ranks) / len(ranks)\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    # Calculate the mean of the mean ranks across all recommendation lists\n",
        "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks)\n",
        "\n",
        "    return mean_of_mean_ranks\n",
        "\n",
        "\n",
        "\n",
        "def mean_average_precision(true_labels, predicted_labels, k=10):\n",
        "    \"\"\"\n",
        "    Calculate the mean Average Precision for a list of recommendations.\n",
        "\n",
        "    Parameters:\n",
        "    true_labels : list of list\n",
        "        True relevant items for each recommendation list.\n",
        "    predicted_labels : list of list\n",
        "        Predicted recommended items for each recommendation list.\n",
        "    k : int\n",
        "        Number of recommendations to consider.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Mean Average Precision value.\n",
        "    \"\"\"\n",
        "    average_precisions = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate Average Precision for each recommendation list\n",
        "        true_set = set(true)\n",
        "        precision_at_k = []\n",
        "        relevant_count = 0\n",
        "        for i, item in enumerate(pred[:k]):\n",
        "            if item in true_set:\n",
        "                relevant_count += 1\n",
        "                precision_at_k.append(relevant_count / (i + 1))\n",
        "        average_precision = sum(precision_at_k) / len(true_set)\n",
        "        average_precisions.append(average_precision)\n",
        "\n",
        "    # Calculate the mean Average Precision\n",
        "    mean_average_precision = sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "    return mean_average_precision\n",
        "\n",
        "def top_k_ranks(citing, cited, cosine_similarities, k=10):\n",
        "    # Create a dictionary to store the top k ranks for each citing patent\n",
        "    top_k_ranks = {}\n",
        "    for i, content_id in enumerate(citing):\n",
        "        top_k_ranks[content_id['id']] = [cited[j]['id'] for j in np.argsort(cosine_similarities[i])[::-1][:k]]\n",
        "    return top_k_ranks\n",
        "\n",
        "\n",
        "def extract_title(dictionary):\n",
        "    return dictionary.get('title', None)  # Return None if 'title' key is not present\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_a8MJUDzyNr"
      },
      "source": [
        "# 1.0 Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0FddbPS7zyNs"
      },
      "outputs": [],
      "source": [
        "json_citing_train = load_json_data(\"./datasets/Content_JSONs/Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TRAIN.json\")\n",
        "json_citing_test = load_json_data(\"./datasets/Content_JSONs/Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TEST.json\")\n",
        "\n",
        "json_nonciting = load_json_data(\"./datasets/Content_JSONs/Cited_2020_Uncited_2010-2019_Cleaned_Content_22k/CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json\")\n",
        "json_citing_to_cited = load_json_data(\"./datasets/Citation_JSONs/Citation_Train.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Urbuut75zyNs"
      },
      "outputs": [],
      "source": [
        "citing_dataset_df = pd.DataFrame(json_citing_train)\n",
        "\n",
        "nonciting_dataset_df = pd.DataFrame(json_nonciting)\n",
        "mapping_dataset_df = pd.DataFrame(json_citing_to_cited)\n",
        "\n",
        "mapping_dict = get_mapping_dict(mapping_dataset_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLoIOWNpzyN9"
      },
      "source": [
        "# Create Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents without description: 0\n",
            "Number of documents without description: 0\n",
            "Number of documents without description: 0\n"
          ]
        }
      ],
      "source": [
        "citing_train_desc = create_corpus(json_citing_train, \"description\")\n",
        "citing_test_desc = create_corpus(json_citing_test, \"description\")\n",
        "\n",
        "nonciting_desc = create_corpus(json_nonciting, \"description\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6831, 1000, 16837)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(citing_train_desc), len(citing_test_desc), len(nonciting_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorizing descriptions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TF-IDF: 100%|██████████| 23668/23668 [01:17<00:00, 306.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary: 97172\n",
            "Shape of citing TF-IDF matrix: (6831, 97172)\n",
            "Shape of citedTF-IDF matrix: (16837, 97172)\n"
          ]
        }
      ],
      "source": [
        "# Best performing vectorizer - 0.4587\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words=stopwords.words('english'),\n",
        "    norm='l2',\n",
        "    min_df=5,\n",
        "    lowercase=True,\n",
        "    use_idf=True,\n",
        "    smooth_idf=True,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "tfidf_matrix_citing_train, tfidf_matrix_nonciting = create_tfidf_matrix(citing_train_desc, nonciting_desc, vectorizer)\n",
        "\n",
        "print(f\"Shape of citing TF-IDF matrix: {tfidf_matrix_citing_train.shape}\")\n",
        "print(f\"Shape of citedTF-IDF matrix: {tfidf_matrix_nonciting.shape}\")\n",
        "\n",
        "cosine_similarities = linear_kernel(tfidf_matrix_citing_train, tfidf_matrix_nonciting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall at 10: 0.6798\n",
            "Recall at 20: 0.7542\n",
            "Recall at 50: 0.8376\n",
            "Recall at 100: 0.8916\n",
            "Mean ranking: 20.3561\n",
            "Mean average precision: 0.4583\n",
            "Number of patents measured: 6831\n",
            "Number of patents not in the citation: 0\n"
          ]
        }
      ],
      "source": [
        "# Get the top k ranks for each citing patent\n",
        "k = 100\n",
        "top_k_rank = top_k_ranks(citing_train_desc, nonciting_desc, cosine_similarities, k=k)\n",
        "\n",
        "# Calculate the metrics\n",
        "true_labels, predicted_labels, not_in_citation_mapping = \\\n",
        "    get_true_and_predicted(mapping_dict, top_k_rank)\n",
        "\n",
        "mean_rank = mean_ranking(true_labels, predicted_labels)\n",
        "mean_average_precision_val = mean_average_precision(true_labels, predicted_labels)\n",
        "recall_at_10_claim1_bm25 = mean_recall_at_k(true_labels, predicted_labels, k=10)\n",
        "recall_at_20_claim1_bm25 = mean_recall_at_k(true_labels, predicted_labels, k=20)\n",
        "recall_at_50_claim1_bm25 = mean_recall_at_k(true_labels, predicted_labels, k=50)\n",
        "recall_at_100_claim1_bm25 = mean_recall_at_k(true_labels, predicted_labels, k=100)\n",
        "\n",
        "print(\"Recall at 10:\", round(recall_at_10_claim1_bm25, 4))\n",
        "print(\"Recall at 20:\", round(recall_at_20_claim1_bm25, 4))\n",
        "print(\"Recall at 50:\", round(recall_at_50_claim1_bm25, 4))\n",
        "print(\"Recall at 100:\", round(recall_at_100_claim1_bm25, 4))\n",
        "\n",
        "\n",
        "print(\"Mean ranking:\", round(mean_rank, 4))\n",
        "print(\"Mean average precision:\", round(mean_average_precision_val, 4))\n",
        "print(\"Number of patents measured:\", len(predicted_labels))\n",
        "print(\"Number of patents not in the citation:\", not_in_citation_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorizing descriptions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TF-IDF: 100%|██████████| 17837/17837 [00:46<00:00, 387.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary: 85165\n",
            "Shape of citing TF-IDF matrix: (6831, 97172)\n",
            "Shape of citedTF-IDF matrix: (16837, 85165)\n"
          ]
        }
      ],
      "source": [
        "# For Evaluation:\n",
        "tfidf_matrix_citing_test, tfidf_matrix_nonciting = create_tfidf_matrix(citing_test_desc, nonciting_desc, vectorizer)\n",
        "\n",
        "print(f\"Shape of citing TF-IDF matrix: {tfidf_matrix_citing_train.shape}\")\n",
        "print(f\"Shape of citedTF-IDF matrix: {tfidf_matrix_nonciting.shape}\")\n",
        "\n",
        "cosine_similarities = linear_kernel(tfidf_matrix_citing_test, tfidf_matrix_nonciting)\n",
        "k = 100\n",
        "top_k_rank = top_k_ranks(citing_test_desc, nonciting_desc, cosine_similarities, k=k)\n",
        "\n",
        "# Save the results to a JSON file\n",
        "with open(\"prediction1.json\", \"w\") as f:\n",
        "    json.dump(top_k_rank, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wC-2H0llzyNk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
